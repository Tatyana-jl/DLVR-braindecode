{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchcontrib.optim import SWA\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy.random import RandomState\n",
    "\n",
    "\n",
    "import braindecode\n",
    "from braindecode.torch_ext.util import np_to_var, var_to_np\n",
    "from braindecode.experiments.monitors import compute_preds_per_trial_from_crops\n",
    "from braindecode.datautil.signal_target import SignalAndTarget\n",
    "from braindecode.datautil.iterators import CropsFromTrialsIterator\n",
    "from braindecode.models.util import to_dense_prediction_model\n",
    "from braindecode.models.eegnet import EEGNetv4\n",
    "from braindecode.torch_ext.optimizers import AdamW\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 1\n",
    "\n",
    "%aimport braindecode.models.deep4, LoadEEG, mne_interface, OnlineToOffline, torchcontrib.optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parameters_data contains information for loader LoadEEG. <br>\n",
    "Used target frequency: 256 Hz <br>\n",
    "time range: from 0.5 sec before trial onset to 3 sec after trial onset <br>\n",
    "filters: highpass filter with cutting frequency 1 Hz, lowpass filter with 30 Hz, notch filter 50 Hz <br>\n",
    "channels: 'eog_eeg' regime returnes eog signals referenced to each other or Fp1/Fp2 channels: <br>\n",
    "\\begin{equation*}\n",
    "result_channel1 = left EOG - right EOG \\\\\n",
    "result_channel2 = Fp1 - down EOG \\\\\n",
    "result_channel3 = Fp2 - up EOG \\\\\n",
    "\\end{equation*}\n",
    "\n",
    "'eeg' - extracts only eeg files <br>\n",
    "'MI' - takes only motor-imagery relevant channels, was used for most of the trained models.<br>\n",
    "LoadEEG.load_subjects() allows to load the data for different subjects, epoch it, preprocess and split them according training mode. <br>\n",
    "For more details please see the code and comments above and function descriptions in the file LoadEEG.py <br>\n",
    "__NOTE__ All models were trained on all data from the subjects (including runs that were considrered as bad according to protocols)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_data = {\n",
    "    'subject_files': ['subjH','subjB','subjD','subjM'], # in case 'leave_one_out' \n",
    "    # training design the data will be organized according to an order in this \n",
    "    # list: the last subject will be used for testing, other - for training and \n",
    "    # validation\n",
    "    'training_design': 'leave_one_out', # 'leave_one_out', 'mix'\n",
    "    'target_fs': 256, # target frequency\n",
    "    'tmin': -0.5, #0.5 seconds before onset of trial\n",
    "    'tmax': 3, #3 seconds after onset of trial\n",
    "    'filters': True, #True/False\n",
    "    'channels': 'MI' #eeg_eog, eeg, MI- motor-imagery relevant channels\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depending on parameter return_only_test=False/True function returns either numpy arrays or SignalAndTarget object\n",
    "train, validation, test, train_labels, validation_labels, test_labels = LoadEEG.load_subjects(parameters_data['subject_files'], training_design=parameters_data['training_design'], target_fs=parameters_data['target_fs'],\n",
    "                  tmin=parameters_data['tmin'], tmax=parameters_data['tmax'], filters=parameters_data['filters'], channels=parameters_data['channels'])\n",
    "\n",
    "train_set = SignalAndTarget(train.copy(), train_labels)\n",
    "validation_set = SignalAndTarget(validation.copy(), validation_labels)\n",
    "test_set = SignalAndTarget(test.copy(), test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check what is in your data (especially if you are using filters)\n",
    "i = 0\n",
    "\n",
    "for channel in range(train_set.X.shape[1]):\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.plot(validation_set.X[-1,channel,:])\n",
    "    \n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.magnitude_spectrum(validation_set.X[-1,channel,:], Fs=256, color='m')\n",
    "    \n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topological plot of variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braindecode.datasets.sensor_positions import get_channelpos, CHANNEL_10_20_APPROX\n",
    "\n",
    "eeg_chan = np.argwhere(ch_types == 'eeg')\n",
    "ch_names=[]\n",
    "for chan in eeg_chan:\n",
    "    ch_names = ch_names + [epochs_left.ch_names[chan[0]]]\n",
    "ch_names = [s.strip('.') for s in ch_names]\n",
    "\n",
    "positions = np.array([get_channelpos(name, CHANNEL_10_20_APPROX) for name in ch_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "lab = {\n",
    "    0: 'left hand',\n",
    "    1: 'right hand'\n",
    "}\n",
    "\n",
    "i=0\n",
    "k=0\n",
    "nr_trials = 10\n",
    "fig, axes = plt.subplots((nr_trials//2),2)\n",
    "fig.set_figwidth(20)\n",
    "fig.set_figheight(50)\n",
    "\n",
    "for trial in range(nr_trials):\n",
    "    ax = axes[i, k]\n",
    "    if k==0:\n",
    "        k+=1\n",
    "    else:\n",
    "        k=0\n",
    "        i+=1\n",
    "    tr = np.random.choice(epochs.shape[0])\n",
    "    tr_label = labels[tr]\n",
    "    trial_var = np.var(epochs[tr,:,:], axis=1)\n",
    "    max_val = np.max(trial_var)\n",
    "    min_val = np.min(trial_var)\n",
    "\n",
    "    mne.viz.plot_topomap(trial_var, positions, vmin=min_val, \n",
    "                         vmax=max_val, contours=0, cmap=cm.coolwarm, axes=ax, show=False)\n",
    "    ax.set_title(lab[tr_label], fontsize=18)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cropped training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section performs a cross-subject (or single subject offline training) <br>\n",
    "*parameters* contains different attributes for training process and also are used to save the model description in .xls file afterwards. <br>\n",
    "**model** - deep4 or eegnet (for comparison): in general you can achieve the same validation performance with deep4 as with eegnet, however the later one is more stable towards change of optimizer and learning rate. <br>\n",
    "**NOTE**: if using Cross Entropy Loss one needs to remove the Softmax layer from the model, also in the original file of model from braindecoder Dropuot layer is switched off (was switched on for these trainings) <br> \n",
    "**input_time_length** - for now defined depending on the train data shape, should be changed in case of extracting the whole trial during epoching to some fix length of sliding window <br>\n",
    "If you use scheduling of learning rate (especially cosine annealing), there are 5 main parametres you should always keep in mind due to the tight connection between them:<br>\n",
    "**n_epoch** - number of epochs for training, **batch_size** - size of minibacth (recommended to take $2^n$ where n - any integer number), **lr** - starting learning rate (recommended to start with >1e-3 for not pretrained models), **eta_min** - min learning rate that scheduler will reach, **optimizer** - three main optimizers were tested. <br>\n",
    "This parameters should always be balanced wrt to each other: <br>\n",
    "SGD+momentum - that gave best results in most of the training ia also much more stable than Adam and AdamW (running with the same parameters several times the same result can be reached), but SGD is slower and it makes sense to use more epochs, that indeed stretches the learning rate function. At the same time learning rate is closely connected (proportionally) to the batch size:the bigger batch size works better with bigger lr and is claimed to lead to better generalization. <br>\n",
    "There are different sete of parameters that were tested, but the most successfull were: <br>\n",
    "**n_epochs**: 500, <br>\n",
    "**batch_size**: 64 <br>\n",
    "**optimizer**: SGD+momentum <br>\n",
    "**lr**: 0.01 <br>\n",
    "**eta_min**: 0.0001 <br>\n",
    "**scheduler**: cosine <br>\n",
    "Additional technique of scheduling was also implemented: Warm Restart. Did not elicit better results though. <br>\n",
    "More information on scheduling techniques and how to work with thme can be found here:<br>\n",
    "https://towardsdatascience.com/https-medium-com-reina-wang-tw-stochastic-gradient-descent-with-restarts-5f511975163 <br>\n",
    "https://pytorch.org/docs/stable/optim.html <br>\n",
    "\n",
    "The best model was trained with early stop regularization: while training the validation loss is tracked and if it is not decreasing after 15 epochs from minimum point, the training is stopped. It also allows you to see how good are your training prameters: if your model stops training too quickly, you should consider tunung parameters again.\n",
    "\n",
    "Also Stochastic Weight Averaging was implemented in a pipeline below. More information on it can be found here: <br>\n",
    "https://pytorch.org/blog/stochastic-weight-averaging-in-pytorch/ <br>\n",
    "https://arxiv.org/abs/1803.05407 <br>\n",
    "It is not clear how promissing this technique would be for this model (waas not tuned), but the code below has all necessary steps for it. <br>\n",
    "**NOTE** after setting weights of the model to SWA weights batch normalization layers also should be updated by seeing all the data again. For this torchcontib.optim.SWA has a function bn_update, but it accepts as an input DataLoader object (not a case for this itterator), to make it still work one needs to edit a function: *b=input.size(0) -> b=input.size* and add a line *input = torch.cuda.FloatTensor(input)*<br>\n",
    "SWA works together with early stop regime: as soon as the best validation loss is achieved, scheduler is stoped and learning rate stays stable while SWA starts to save gradients further with training and does it cerain amount of time till the end. <br>\n",
    "\n",
    "Evaluation: ther are plots of loss function (training and validation) and accuracy which allow to see the behaviour of the model, however the most interesting evaluation in this case is how good the model can perform on new unseen data from new subject so it should also be a valuable criterion for choosing a model for further online testing. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that defines number of crops per supercrop for a certain model and input signal (model should be in dense regime)\n",
    "def preds_per_input(model, net_input):\n",
    "    net_input = Variable(torch.cuda.FloatTensor(net_input[:,:,:,None])) if torch.cuda.is_available() else Variable(torch.FloatTensor(net_input[:,:,:,None]))\n",
    "    try:\n",
    "        return model(net_input.cuda()).shape[2]\n",
    "    except IndexError:\n",
    "        print('Incorrect shape of output, check if network is in dense mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    # network \n",
    "    'model': 'eegnet', # 'deep4', 'eegnet'\n",
    "    'input_time_length': train_set.X.shape[2], #length of trial\n",
    "    'in_chans': train_set.X.shape[1], #number of channels\n",
    "    'n_classes': 2, # number of classes\n",
    "    #training\n",
    "    'n_epochs': 500, # number of epochs to train\n",
    "    'batch_size': 64, #size of minibatch\n",
    "    'criterion': 'cross_entr', # loss function\n",
    "    'early_stop': 15, # patients for early stop, False - when no early stop is needed\n",
    "    # optimizer\n",
    "    'optimizer': 'SGD', # 'Adam', 'AdamW', 'SGD' - SGD+momentum\n",
    "    'sched': True, # True/False - learning rate scheduling\n",
    "    'sched_type': 'cosine', # type of scheduling (for the records, does not change scheduling automathically)\n",
    "    'lr': 0.01, # learning rate (starting learning rate in case of scheduling)\n",
    "    'b1': 0.9, # b1 parameters for Adam amd AdamW\n",
    "    'b2': 0.999, # b2 parameter for Adam and AdamW\n",
    "    'eta_min': 0.0001, #minimum learning rate for scheduling\n",
    "    'Tmax': None, #number of steps for scheduler (if sched=True this parameter is calculated later)\n",
    "    'WR': False, # True/False - Warm Restart switch\n",
    "    'restart_lr': 50, # number of epochs to wait till restarting learning rate\n",
    "    'SWA': False, # True/False - Stochastic Weight Averaging switch\n",
    "    'n_swa': 20, # number of savings for SWA (in fact - a bit less)\n",
    "    'cuda': True # True/False - cuda \n",
    "}\n",
    "\n",
    "# Create model \n",
    "\n",
    "cuda = parameters['cuda']\n",
    "\n",
    "if parameters['model'] == 'deep4': \n",
    "    model_cropp = braindecode.models.deep4.Deep4Net(in_chans=parameters['in_chans'], \n",
    "                                                    n_classes=parameters['n_classes'],\n",
    "                                                    input_time_length=parameters['input_time_length'], \n",
    "                                                    final_conv_length=1, \n",
    "                                                    batch_norm_alpha=0.1).create_network()\n",
    "    to_dense_prediction_model(model_cropp)\n",
    "    model_cropp.cuda() if cuda else None\n",
    "\n",
    "elif parameters['model'] == 'eegnet':\n",
    "    model_cropp = EEGNetv4(in_chans=parameters['in_chans'], \n",
    "                           n_classes=parameters['n_classes'], \n",
    "                           final_conv_length=1, \n",
    "                           input_time_length=parameters['input_time_length']).create_network()\n",
    "    to_dense_prediction_model(model_cropp)\n",
    "    model_cropp.cuda()\n",
    "    \n",
    "n_preds_per_input = preds_per_input(model_cropp, train_set.X)\n",
    "\n",
    "\n",
    "#Create iterator\n",
    "iterator = CropsFromTrialsIterator(batch_size=parameters['batch_size'],input_time_length=parameters['input_time_length'],\n",
    "                                  n_preds_per_input=n_preds_per_input)\n",
    "\n",
    "n_updates_per_epoch = len([None for b in iterator.get_batches(train_set, True)])\n",
    "\n",
    "# Additional settings for warm restart, annealing and SWA\n",
    "if parameters['WR']:\n",
    "    parameters['Tmax'] = parameters['restart_lr'] * n_updates_per_epoch\n",
    "else:\n",
    "    parameters['Tmax'] = parameters['n_epochs'] * n_updates_per_epoch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create database for today's recording \n",
    "try:\n",
    "    todayRecord\n",
    "    if input('Are you sure you want to rewrite this database?') == 'yes':\n",
    "        name_database = datetime.date.today()\n",
    "        col = list(parameters_data.keys()) + list(parameters.keys()) + ['temporal filter size','min loss', 'epochs final', 'acc train', 'acc validation', 'acc test', 'curve file', 'model file']\n",
    "        todayRecord = pd.DataFrame(columns = col)\n",
    "except NameError:\n",
    "    name_database = datetime.date.today()\n",
    "    col = list(parameters_data.keys()) + list(parameters.keys()) + ['temporal filter size','min loss', 'epochs final', 'acc train', 'acc validation', 'acc test', 'curve file', 'model file']\n",
    "    todayRecord = pd.DataFrame(columns = col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set optimizer\n",
    "if parameters['optimizer']=='AdamW':\n",
    "    optimizer = AdamW(model_cropp.parameters(), \n",
    "                      lr = parameters['lr'], \n",
    "                      weight_decay=5*1e-5)\n",
    "    print('Optimizer: AdamW')\n",
    "if parameters['optimizer']=='Adam':\n",
    "    optimizer = torch.optim.Adam(model_cropp.parameters(), \n",
    "                                 lr = parameters['lr'],\n",
    "                                 betas = (parameters['b1'],\n",
    "                                 parameters['b2']), \n",
    "                                 weight_decay = 1e-5)\n",
    "    print('Optimizer: Adam')\n",
    "if parameters['optimizer']=='SGD':\n",
    "    optimizer = torch.optim.SGD(model_cropp.parameters(), \n",
    "                                lr=parameters['lr'],\n",
    "                                momentum=0.9, \n",
    "                                weight_decay = 1e-5, \n",
    "                                nesterov=True)\n",
    "    print('Optimizer: SGD+momentum')\n",
    "\n",
    "if parameters['sched']:\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, \n",
    "                                                           T_max=parameters['Tmax'],\n",
    "                                                           eta_min=parameters['eta_min'], \n",
    "                                                           last_epoch=-1)\n",
    "    print('Schedule learning rate')\n",
    "else:\n",
    "    print(\"No scheduling\")\n",
    "\n",
    "    \n",
    "if parameters['criterion']=='cross_entr':\n",
    "    criterion=torch.nn.CrossEntropyLoss().cuda()\n",
    "    print(\"Loss function: cross entropy\")\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if torch.cuda.is_available() else torch.LongTensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## housekeeping ##\n",
    "\n",
    "train_loss = np.array([])\n",
    "val_loss = np.array([])\n",
    "\n",
    "train_acc = np.array([])\n",
    "val_acc = np.array([])\n",
    "\n",
    "lr = np.array([])\n",
    "cross_points = np.array([])\n",
    "\n",
    "min_loss = np.inf\n",
    "early_stop = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "# when training_mode is 'leave_one_out' network never sees the test data. \n",
    "# However, if one wants to cjeck how network would perform on test data having a \n",
    "# chunk of it in offline training process parameter check_network_test will add some \n",
    "#test trial to training and validation sets\n",
    "check_network_test = False\n",
    "\n",
    "if check_network_test:\n",
    "    train_set.X = np.concatenate((train_set.X, test_set.X[:50,:,:]), axis=0)\n",
    "    train_set.y = np.concatenate((train_set.y, test_set.y[:50]), axis=0)\n",
    "\n",
    "    test_set.X = test_set.X[50:,:,:]\n",
    "    test_set.y = test_set.y[50:]\n",
    "\n",
    "    validation_set.X = np.concatenate((validation_set.X, test_set.X[:20,:,:]), axis=0)\n",
    "    validation_set.y = np.concatenate((validation_set.y, test_set.y[:20]), axis=0)\n",
    "\n",
    "    test_set.X = test_set.X[20:,:,:]\n",
    "    test_set.y = test_set.y[20:]\n",
    "else:\n",
    "    train_set = SignalAndTarget(train.copy(), train_labels)\n",
    "    validation_set = SignalAndTarget(validation.copy(), validation_labels)\n",
    "    test_set = SignalAndTarget(test.copy(), test_labels)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "n_step = 0\n",
    "\n",
    "for i_epoch in range(parameters['n_epochs']):\n",
    "\n",
    "    print(\"Epoch {:d}\".format(i_epoch))\n",
    "\n",
    "    model_cropp.train()\n",
    "    \n",
    "    for batch_X, batch_y in iterator.get_batches(train_set, shuffle=True):\n",
    "        \n",
    "        net_in = Variable(FloatTensor(batch_X))\n",
    "        net_target = Variable(LongTensor(batch_y))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_cropp(net_in)\n",
    "        outputs = torch.mean(outputs, dim=2, keepdim=False)\n",
    "\n",
    "        loss = criterion(outputs, net_target)        \n",
    "        loss.backward()\n",
    "        optimizer.step()  \n",
    "        \n",
    "        if parameters['sched']:\n",
    "            scheduler.step()\n",
    "        \n",
    "        lr = np.append(lr, optimizer.state_dict()['param_groups'][0]['lr'])            \n",
    "        \n",
    "        n_step += 1\n",
    "\n",
    "    if (parameters['WR'])&(i_epoch>0)&(i_epoch%parameters['restart_lr']==0): # for warm restart\n",
    "\n",
    "        optimizer.optimizer.param_groups[0]['lr'] = parameters['lr']\n",
    "        \n",
    "        if parameters['sched']:\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "                optimizer, T_max=parameters['Tmax'], \n",
    "                eta_min=parameters['eta_min'], last_epoch=-1)\n",
    "        \n",
    "    outputs = outputs.detach().cpu().numpy()\n",
    "    train_loss = np.append(train_loss, loss.cpu().detach().numpy())\n",
    "    accuracy = np.mean(np.argmax(outputs, axis = 1) == batch_y)\n",
    "    train_acc = np.append(train_acc, accuracy)\n",
    "    \n",
    "    print(\"{:6s} Loss: {:.5f}\".format('Train', loss))\n",
    "    print(\"{:6s} Accuracy: {:.1f}%\".format('Train', accuracy * 100))\n",
    "\n",
    "    model_cropp.eval()\n",
    "\n",
    "    net_in = Variable(FloatTensor(validation_set.X[:,:,:,None]), requires_grad=False)\n",
    "    net_target = Variable(LongTensor(validation_set.y), requires_grad=False)\n",
    "\n",
    "    outputs = model_cropp(net_in)\n",
    "    outputs = torch.mean(outputs, dim=2, keepdim=False)\n",
    "\n",
    "    loss = criterion(outputs, net_target)\n",
    "    loss = loss.cpu().detach().numpy()\n",
    "    if parameters['early_stop'] & (loss<min_loss):\n",
    "        min_loss = loss\n",
    "        save_loss = train_loss[-1]\n",
    "        early_stop = 0\n",
    "        model = model_cropp\n",
    "    else:\n",
    "        early_stop+=1\n",
    "\n",
    "    outputs = outputs.detach().cpu().numpy()\n",
    "    accuracy = np.mean(np.argmax(outputs, axis = 1) == validation_set.y)\n",
    "\n",
    "    print(\"{:6s} Loss: {:.5f}\".format('Valid', loss))\n",
    "    print(\"{:6s} Accuracy: {:.1f}%\".format(\n",
    "        'Valid', accuracy * 100))\n",
    "\n",
    "    val_loss = np.append(val_loss, loss)    \n",
    "    val_acc = np.append(val_acc, accuracy)\n",
    "\n",
    "    if parameters['early_stop'] & (early_stop == parameters['early_stop']):\n",
    "        print('Early stop triggered')\n",
    "        if parameters['SWA']:\n",
    "            parameters['swa_freq'] = n_updates_per_epoch*(parameters['n_epochs']-i_epoch)//parameters['n_swa']\n",
    "            parameters['sched'] = False\n",
    "            optimizer = SWA(optimizer, swa_start=0, \n",
    "                            swa_lr=optimizer.param_groups[0]['lr'], \n",
    "                            swa_freq=parameters['swa_freq'])\n",
    "        else:\n",
    "            model_cropp = model\n",
    "            break\n",
    "\n",
    "            \n",
    "end = time.time()\n",
    "print('%d epochs executed in %f seconds' % (i_epoch+1,end-start))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script bellow allows to retrain model with all the data after early stopping to a certain training loss that was saved in previous training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "break_train = 0\n",
    "\n",
    "train_set.X = np.concatenate((train_set.X, validation_set.X), axis=0)\n",
    "train_set.y = np.concatenate((train_set.y, validation_set.y), axis=0)\n",
    "\n",
    "start = time.time()\n",
    "i_epoch = 0\n",
    "\n",
    "for i_epoch in range(parameters['n_epochs']):\n",
    "\n",
    "    print(\"Epoch {:d}\".format(i_epoch))\n",
    "\n",
    "    model_cropp.train()\n",
    "    for batch_X, batch_y in iterator.get_batches(train_set, shuffle=True):\n",
    "\n",
    "        net_in = Variable(FloatTensor(batch_X))\n",
    "        net_target = Variable(LongTensor(batch_y))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_cropp(net_in)\n",
    "        outputs = torch.mean(outputs, dim=2, keepdim=False)\n",
    "    \n",
    "        loss = criterion(outputs, net_target)   \n",
    "        \n",
    "        if loss<save_loss:\n",
    "            print('Reached the loss of training')\n",
    "            break_train = True\n",
    "            break\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        if parameters['sched']:\n",
    "            scheduler.step()\n",
    "    \n",
    "    if break_train:\n",
    "        break\n",
    "\n",
    "\n",
    "    if (parameters['WR'])&(i_epoch%parameters['restart_lr']==0): # for warm restart\n",
    "\n",
    "        if parameters['optimizer']=='AdamW':\n",
    "            optimizer = AdamW(model_cropp.parameters(), \n",
    "                          lr = parameters['lr'], weight_decay=5*1e-4)\n",
    "\n",
    "        if parameters['optimizer']=='Adam':\n",
    "            optimizer = torch.optim.Adam(model_cropp.parameters(), lr = parameters['lr'], \n",
    "                                 betas = (parameters['b1'], parameters['b2']), weight_decay = 0)\n",
    "\n",
    "        if parameters['sched']:\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "                optimizer, T_max=parameters['Tmax'], \n",
    "                eta_min=parameters['eta_min'], last_epoch=-1)\n",
    "\n",
    "    if i_epoch%check_epoch == 0:\n",
    "        \n",
    "        outputs = outputs.detach().cpu().numpy()\n",
    "        train_loss = np.append(train_loss, loss.cpu().detach().numpy())\n",
    "        accuracy = np.mean(np.argmax(outputs, axis = 1) == batch_y)\n",
    "        train_acc = np.append(train_acc, accuracy)\n",
    "        print(\"{:6s} Loss: {:.5f}\".format('Train', loss))\n",
    "        print(\"{:6s} Accuracy: {:.1f}%\".format('Train', accuracy * 100))\n",
    "\n",
    "        model_cropp.eval()\n",
    "\n",
    "        net_in = Variable(FloatTensor(validation_set.X[:,:,:,None]), requires_grad=False)\n",
    "        net_target = Variable(LongTensor(validation_set.y), requires_grad=False)\n",
    "\n",
    "        outputs = model_cropp(net_in)\n",
    "        outputs = torch.mean(outputs, dim=2, keepdim=False)\n",
    "\n",
    "        loss = criterion(outputs, net_target)\n",
    "        loss = loss.cpu().detach().numpy()\n",
    "    \n",
    "        outputs = outputs.detach().cpu().numpy()\n",
    "        accuracy = np.mean(np.argmax(outputs, axis = 1) == validation_set.y)\n",
    "        \n",
    "        lr = np.append(lr, optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "\n",
    "        print(\"{:6s} Loss: {:.5f}\".format('Valid', loss))\n",
    "        print(\"{:6s} Accuracy: {:.1f}%\".format(\n",
    "            'Valid', accuracy * 100))\n",
    "\n",
    "        val_loss = np.append(val_loss, loss)    \n",
    "        val_acc = np.append(val_acc, accuracy)\n",
    "            \n",
    "            \n",
    "end = time.time()\n",
    "print('%d epochs executed in %f seconds' % (i_epoch+1,end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_im =False\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(train_loss[3:], label='training loss')\n",
    "plt.plot(val_loss[3:], label='validation loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.title('Loss function')\n",
    "plt.legend()\n",
    "\n",
    "if input('Do you want to save images for this training?') == 'yes':\n",
    "    save_im = True\n",
    "    now = datetime.datetime.now()\n",
    "    fig_name = str(now.day)+'-'+str(now.month)+'_'+str(now.hour)+'-'+str(now.minute)\n",
    "    plt.savefig(fig_name+'_loss')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(lr, label='learning_rate')\n",
    "plt.scatter(cross_points.astype(np.int), lr[cross_points.astype(np.int)], marker='x', color='r', label='SWA')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('lr')\n",
    "plt.title('Learning rate')\n",
    "plt.legend()\n",
    "if save_im:\n",
    "    plt.savefig(fig_name+'_lr')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(train_acc, label='training accuracy')\n",
    "plt.plot(val_acc, label='validation accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "if save_im:\n",
    "    plt.savefig(fig_name+'_accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test ###\n",
    "print('Best validation accuracy:', np.around(np.max(val_acc),2))\n",
    "model_cropp.eval()\n",
    "\n",
    "net_in = Variable(FloatTensor(test_set.X[:,:,:,None]), requires_grad=False)\n",
    "net_target = Variable(LongTensor(test_set.y), requires_grad=False)\n",
    "\n",
    "output_test = model_cropp(net_in)\n",
    "output_test = torch.mean(output_test, dim=2, keepdim=False)\n",
    "\n",
    "loss = F.nll_loss(output_test, net_target)\n",
    "print('Test loss:', loss.cpu().detach().numpy())\n",
    "\n",
    "predicted_labels = np.argmax(var_to_np(output_test), axis=1)\n",
    "accuracy = np.mean(test_set.y  == predicted_labels)\n",
    "print('Test accuracy:', np.around(accuracy,2))\n",
    "\n",
    "\n",
    "if parameters['SWA']:\n",
    "    \n",
    "    optimizer.swap_swa_sgd()\n",
    "    optimizer.bn_update(iterator.get_batches(train_set, shuffle=True), model_cropp)\n",
    "    \n",
    "    output_test = model_cropp(net_in)\n",
    "    output_test = torch.mean(output_test, dim=2, keepdim=False)\n",
    "\n",
    "    loss = F.nll_loss(output_test, net_target)\n",
    "    print('Test loss with SWA:', loss.cpu().detach().numpy())\n",
    "\n",
    "    predicted_labels = np.argmax(var_to_np(output_test), axis=1)\n",
    "    accuracy_swa = np.mean(test_set.y  == predicted_labels)\n",
    "    print('Test accuracy with SWA:', np.around(accuracy_swa,2))\n",
    "\n",
    "    \n",
    "if input('Do you want me to save the model description?') == 'yes':\n",
    "    fill_line = todayRecord.shape[0]\n",
    "    for param in parameters_data:\n",
    "        todayRecord.at[fill_line, param] = parameters_data[param]\n",
    "    for param in parameters:\n",
    "        todayRecord.at[fill_line, param] = parameters[param]\n",
    "    todayRecord.at[fill_line, 'temporal filter size'] = model_cropp[1].weight.shape[2]\n",
    "    todayRecord.at[fill_line, 'min loss'] = min_loss\n",
    "    todayRecord.at[fill_line, 'epochs final'] = i_epoch+1\n",
    "    todayRecord.at[fill_line, 'acc train'] = np.max(train_acc)\n",
    "    todayRecord.at[fill_line, 'acc validation'] = np.max(val_acc)\n",
    "    todayRecord.at[fill_line, 'acc test'] = np.around(accuracy,2)\n",
    "    \n",
    "    if parameters['SWA']:\n",
    "        todayRecord.at[fill_line, 'acc test with SWA'] = np.around(accuracy_swa,2)\n",
    "    \n",
    "    if save_im:\n",
    "        todayRecord.at[fill_line, 'curve file'] = fig_name\n",
    "    else:\n",
    "        todayRecord.at[fill_line, 'curve file'] = 'no'\n",
    "    \n",
    "    if input('Do you want to save the model?') == 'yes':\n",
    "        name_model = input('Name this model please:')\n",
    "        torch.save(model_cropp, name_model)\n",
    "        print('This model is saved')\n",
    "        todayRecord.at[fill_line, 'model file'] = name_model\n",
    "    \n",
    "    print('All saved and you can keep training. You got it!')\n",
    "\n",
    "        \n",
    "    todayRecord.to_excel(str(name_database)+'1'+'.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time_filters = np.squeeze(model_cropp[1].weight.detach().cpu().numpy())\n",
    "nr=0\n",
    "for filt in time_filters:\n",
    "    nr+=1\n",
    "    plt.figure()\n",
    "    plt.plot(filt)\n",
    "    plt.title('kernel of the filter '+ str(nr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo-online training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is designed to perform pseudo-onlne training on unseen data) <br>\n",
    "Batch size should again correspond to the learing rate, size of 8 with learning rate 1e-3 or 1e-4 seems to gain the best results for tested models.<br>\n",
    "It is in geenral hard to make the network to learn on new data from first couple of trials, however, if the pretrained model is good enough, your accuracy can vary from 55 to 70 percents during the online run. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_online = {\n",
    "    'model file': ['model_01-07_10-29', 'model_01-07_10-11', 'model_25-06_19-23', 'model_19-06_11-16'],\n",
    "    # list of models' files (make sure the order is the same as for the test_subject_data:\n",
    "    # each subject correspond to the model that has never seen that data)\n",
    "    'test subject files': ['subjB', 'subjD', 'subjH', 'subjM'], # list of files with subjects' data\n",
    "    'training layers': [1,2,26], # all other layers except these will be frozen\n",
    "    # training\n",
    "    'batch_size': 8, # size of the minibatch\n",
    "    # optimizer\n",
    "    'optimizer': 'SGD', # optimizer\n",
    "    'lr': 1e-3, # learning rate\n",
    "    'b1': 0.9,\n",
    "    'b2': 0.999,\n",
    "    'cuda': True\n",
    "}\n",
    "\n",
    "criterion=torch.nn.CrossEntropyLoss().cuda()\n",
    "print(\"Loss function: cross entropy\")\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if torch.cuda.is_available() else torch.LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test subjects' data\n",
    "test_subject_data = {}\n",
    "for subject in parameters_online['test subject files']:\n",
    "    print(subject)\n",
    "    test_subject_data[subject] = LoadEEG.load_subjects([subject], training_design=parameters_data['training_design'], target_fs=parameters_data['target_fs'],\n",
    "                                                       tmin=parameters_data['tmin'], tmax=parameters_data['tmax'], filters=parameters_data['filters'], channels=parameters_data['channels'], return_only_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create database for today's online recording \n",
    "try:\n",
    "    todayRecord_online\n",
    "    if input('Are you sure you want to rewrite this database?') == 'yes':\n",
    "        name_database = datetime.date.today()\n",
    "        col = list(parameters_online.keys()) + ['curve file']\n",
    "        todayRecord_online = pd.DataFrame(columns = col)\n",
    "except NameError:\n",
    "    name_database = datetime.date.today()\n",
    "    col = list(parameters_online.keys()) + ['curve files']\n",
    "    todayRecord_online = pd.DataFrame(columns = col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training for every model on corresponding test data\n",
    "\n",
    "figure = plt.figure(figsize=(20,20))\n",
    "ax_train_loss = figure.add_subplot(411)\n",
    "ax_loss = figure.add_subplot(412)\n",
    "ax_success_rate = figure.add_subplot(413)\n",
    "ax_accuracy = figure.add_subplot(414)\n",
    "\n",
    "test_input = Variable(FloatTensor(np.ones((2, test_subject_data[subject].X.shape[1], \n",
    "                                           test_subject_data[subject].X.shape[2], 1), dtype=np.float32)))\n",
    "if torch.cuda.is_available():\n",
    "    test_input = test_input.cuda()\n",
    "out = model_cropp(test_input)\n",
    "n_preds_per_input = out.shape[2]\n",
    "\n",
    "\n",
    "# if True, the average performance is shown (meaning \n",
    "# that training will run on the minimum amount of trials among subjects), if False - \n",
    "# each subject's data will be used almost completely for training\n",
    "average_mode = True\n",
    "\n",
    "if average_mode:\n",
    "    min_trials = np.min([test_subject_data[subject].X.shape[0] for \n",
    "                         subject in parameters_online['test subject files']])\n",
    "    line_opaque = 0.3\n",
    "    try:\n",
    "        del avg_train_loss, avg_train_accum_loss, avg_online_loss, avg_online_acc, avg_online_accum_acc, avg_success_rate\n",
    "    except NameError:\n",
    "        None\n",
    "else:\n",
    "    min_trials = None\n",
    "    line_opaque = 1\n",
    "    \n",
    "for i_model, model in enumerate(parameters_online['model file']):\n",
    "    \n",
    "    # Set model\n",
    "    model_cropp = torch.load(model)\n",
    "\n",
    "    for ind, layer in enumerate(model_cropp):\n",
    "        if ind not in parameters_online['training layers']:\n",
    "            if hasattr(layer, 'weight'):\n",
    "                if hasattr(layer.weight, 'requires_grad'):\n",
    "                    layer.weight.requires_grad = False\n",
    "            if hasattr(layer, 'bias'):\n",
    "                if hasattr(layer.bias, 'requires_grad'):\n",
    "                    layer.bias.requires_grad = False\n",
    "                    \n",
    "    #Set optimizer\n",
    "    if parameters_online['optimizer']=='AdamW':\n",
    "        optimizer = AdamW(model_cropp.parameters(), \n",
    "                          lr = parameters_online['lr'], weight_decay=0)\n",
    "        print('Optimizer: AdamW')\n",
    "    if parameters_online['optimizer']=='Adam':\n",
    "        optimizer = torch.optim.Adam(model_cropp.parameters(), lr = parameters_online['lr'], \n",
    "                                 betas = (parameters_online['b1'], parameters_online['b2']), weight_decay = 0)\n",
    "        print('Optimizer: Adam')\n",
    "    if parameters_online['optimizer']=='SGD':\n",
    "        optimizer = torch.optim.SGD(model_cropp.parameters(), lr=parameters_online['lr'], \n",
    "                                momentum=0.9, weight_decay = 1e-5, nesterov=True)\n",
    "        print('Optimizer: SGD+momentum')\n",
    "\n",
    "    if parameters_online['sched']:\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=parameters_online['Tmax'], eta_min=parameters_online['eta_min'], last_epoch=-1)\n",
    "        print('Schedule learning rate')\n",
    "    else:\n",
    "        print(\"No scheduling\")\n",
    "    \n",
    "    # Run training\n",
    "    \n",
    "    train_loss, train_accum_loss, online_loss, online_acc, online_accum_acc, success_rate = train_pseudo_online(model_cropp, \n",
    "                                                                                                                test_subject_data[parameters_online['test subject files'][i_model]],\n",
    "                                                                                                                average_mode,\n",
    "                                                                                                                min_trials)\n",
    "    print(parameters_online['test subject files'][i_model]) \n",
    "    \n",
    "    ax_train_loss.plot(train_loss, color='grey', linewidth=1, alpha=line_opaque)\n",
    "    ax_loss.plot(train_accum_loss, color='blue', linewidth=1, alpha=line_opaque)\n",
    "    ax_loss.plot(online_loss, color='red', linewidth=1, alpha=line_opaque)\n",
    "    ax_success_rate.plot(success_rate, color='grey', linewidth=1, alpha=line_opaque)\n",
    "    ax_accuracy.plot(online_accum_acc, color='blue', linewidth=1, alpha=line_opaque)\n",
    "    ax_accuracy.plot(online_acc, color='red', linewidth=1, alpha=line_opaque)\n",
    "    \n",
    "    if average_mode:\n",
    "        try:\n",
    "            avg_train_loss = np.mean([avg_train_loss, train_loss], axis=0)\n",
    "            avg_train_accum_loss = np.mean([avg_train_accum_loss, train_accum_loss], axis=0)\n",
    "            avg_online_loss = np.mean([avg_online_loss, online_loss], axis=0)\n",
    "            avg_online_acc = np.mean([avg_online_acc, online_acc], axis=0)\n",
    "            avg_online_accum_acc = np.mean([avg_online_accum_acc, online_accum_acc], axis=0)\n",
    "            avg_success_rate = np.mean([avg_success_rate, success_rate], axis=0)\n",
    "        except NameError:\n",
    "            avg_train_loss = train_loss\n",
    "            avg_train_accum_loss = train_accum_loss\n",
    "            avg_online_loss = online_loss\n",
    "            avg_online_acc = online_acc\n",
    "            avg_online_accum_acc = online_accum_acc\n",
    "            avg_success_rate = success_rate\n",
    "        \n",
    "\n",
    "save_im = False\n",
    "if input('Do you want to save images for this training?') == 'yes':\n",
    "    save_im = True\n",
    "    now = datetime.datetime.now()\n",
    "    fig_name = str(now.day)+'-'+str(now.month)+'_'+str(now.hour)+'-'+str(now.minute)\n",
    "    \n",
    "if average_mode:\n",
    "    ax_train_loss.plot(avg_train_loss, color='orange', linewidth=2)\n",
    "    ax_loss.plot(avg_online_loss, label='loss on remaining data', color='r', linewidth=4)\n",
    "    ax_loss.plot(avg_train_accum_loss, label='accumulative training loss', color='b', linewidth=4)\n",
    "    ax_success_rate.plot(avg_success_rate, color='orange', linewidth=2)\n",
    "    ax_accuracy.plot(avg_online_acc, color='r', label='accuracy on remaining data', linewidth=4)\n",
    "    ax_accuracy.plot(avg_online_accum_acc, color='b', label ='accumulative accuracy', linewidth=4)\n",
    "    ax_loss.legend()\n",
    "    ax_accuracy.legend()\n",
    "\n",
    "ax_train_loss.set_xlabel('training step')\n",
    "ax_train_loss.set_title('Loss during online training')\n",
    "\n",
    "    \n",
    "ax_loss.set_xlabel('training step')\n",
    "ax_loss.set_title('Loss on training and remaining data')\n",
    "\n",
    "ax_success_rate.set_xlabel('training step')\n",
    "ax_success_rate.set_title('Success rate')\n",
    "\n",
    "\n",
    "ax_accuracy.set_xlabel('training step')\n",
    "ax_accuracy.set_title('Accuracy on training and remaining data')\n",
    "\n",
    "\n",
    "if save_im:\n",
    "    plt.savefig(fig_name+'_training curves')\n",
    "\n",
    "\n",
    "\n",
    "if input('Do you want me to save the training description?') == 'yes':\n",
    "    fill_line = todayRecord_online.shape[0]\n",
    "    for param in parameters_online:\n",
    "        todayRecord_online.at[fill_line, param] = parameters_online[param]    \n",
    "    if save_im:\n",
    "        todayRecord_online.at[fill_line, 'curve file'] = fig_name\n",
    "    else:\n",
    "        todayRecord_online.at[fill_line, 'curve file'] = 'no'\n",
    "    \n",
    "    print('All saved and you can keep training. You got it!')\n",
    "\n",
    "        \n",
    "    todayRecord_online.to_excel(str(name_database)+'_online'+'.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for training\n",
    "\n",
    "def train_pseudo_online(model_cropp, test_set, averaged_mode, min_trial):\n",
    "    train_loss = np.array([])\n",
    "    train_accum_loss = np.array([])\n",
    "    online_loss = np.array([])\n",
    "    online_acc = np.array([])\n",
    "    online_accum_acc = np.array([])\n",
    "    success_rate = np.array([])\n",
    "\n",
    "    iterator_online = CropsFromTrialsIterator(batch_size=parameters_online['batch_size'],input_time_length=test_set.X.shape[2],\n",
    "                                  n_preds_per_input=n_preds_per_input)\n",
    "    \n",
    "    if averaged_mode:\n",
    "        skip_trials = test_set.X.shape[0] - min_trial\n",
    "    else:\n",
    "        skip_trials = 20\n",
    "    \n",
    "    print('Total test trials for subject:', test_set.X.shape[0])\n",
    "    print('Left unseen trials:', skip_trials)\n",
    "    for trial in range(parameters_online['batch_size'],test_set.X.shape[0]-skip_trials,parameters_online['batch_size']):\n",
    "\n",
    "        test_data_input = test_set.X[:trial,:,:]\n",
    "        test_label_input = test_set.y[:trial]\n",
    "        test_set_input = SignalAndTarget(test_data_input, test_label_input)\n",
    "\n",
    "        model_cropp.train()\n",
    "\n",
    "        net_in = Variable(FloatTensor(test_set_input.X[:, :, :, None]))\n",
    "        net_target = Variable(LongTensor(test_set_input.y))\n",
    "        outputs = model_cropp(net_in)\n",
    "        outputs = torch.mean(outputs, dim=2, keepdim=False)\n",
    "        loss = criterion(outputs, net_target) \n",
    "        train_accum_loss = np.append(train_accum_loss, loss.detach().cpu().numpy())\n",
    "        online_accum_acc = np.append(online_accum_acc, \n",
    "                                     np.mean(np.argmax(outputs.detach().cpu().numpy(), axis = 1) == test_label_input))\n",
    "\n",
    "        for batch_X, batch_y in iterator_online.get_batches(test_set_input, shuffle=True):\n",
    "    #         lr = np.append(lr, optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "\n",
    "            net_in = Variable(FloatTensor(batch_X))\n",
    "            net_target = Variable(LongTensor(batch_y))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model_cropp(net_in)\n",
    "            outputs = torch.mean(outputs, dim=2, keepdim=False)\n",
    "\n",
    "            loss = criterion(outputs, net_target) \n",
    "            loss.backward()\n",
    "            optimizer.step()      \n",
    "            if parameters_online['sched']:\n",
    "                scheduler.step()\n",
    "            outputs = outputs.detach().cpu().numpy()\n",
    "            success_rate = np.append(success_rate, \n",
    "                                     np.mean(np.argmax(outputs, axis = 1) == batch_y ))\n",
    "\n",
    "            train_loss = np.append(train_loss, loss.detach().cpu().numpy())\n",
    "\n",
    "        #check on remaining data\n",
    "        model_cropp.eval()\n",
    "\n",
    "        leftovers_X, leftovers_y = test_set.X[trial:,:,:,None], test_set.y[trial:]\n",
    "\n",
    "        net_in = Variable(FloatTensor(leftovers_X), requires_grad=False)\n",
    "        net_target = Variable(LongTensor(leftovers_y), requires_grad=False)\n",
    "\n",
    "        outputs = model_cropp(net_in)\n",
    "        outputs = torch.mean(outputs, dim=2, keepdim=False)\n",
    "\n",
    "        loss = criterion(outputs, net_target)\n",
    "\n",
    "        loss = loss.cpu().detach().numpy()\n",
    "        outputs = outputs.detach().cpu().numpy()\n",
    "\n",
    "        accuracy = np.mean(np.argmax(outputs, axis = 1) == leftovers_y)\n",
    "\n",
    "        online_loss = np.append(online_loss, loss)    \n",
    "        online_acc = np.append(online_acc, accuracy)\n",
    "        \n",
    "    return train_loss, train_accum_loss, online_loss, online_acc, online_accum_acc, success_rate\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test  savings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from braindecode.torch_ext.constraints import MaxNormDefaultConstraint\n",
    "sys.path.insert(0, '/home/tanja/braindecode-online/bdonline')\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 1\n",
    "\n",
    "%aimport trainers, braindecode.models.deep4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_cropp = torch.load('model_19-06_11-16')\n",
    "optimizer = AdamW(model_cropp.parameters(), \n",
    "                  lr = parameters['lr'], weight_decay=0)\n",
    "trainer = trainers.BatchCntTrainer(model=model_cropp, \n",
    "                                   loss_function=criterion, \n",
    "                                   model_loss_function=None, \n",
    "                                   model_constraint = MaxNormDefaultConstraint(), \n",
    "                                   optimizer=optimizer, \n",
    "                                   input_time_length=parameters['input_time_length'], \n",
    "                                   n_preds_per_input=n_preds_per_input, \n",
    "                                   n_classes=parameters['n_classes'], \n",
    "                                   n_updates_per_break=5, \n",
    "                                   batch_size=30,\n",
    "                                   n_min_trials=10, \n",
    "                                   trial_start_offset=500, \n",
    "                                   break_start_offset=1000, \n",
    "                                   break_stop_offset=-1000,\n",
    "                                   savegrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list = np.array([])\n",
    "\n",
    "for batch_X, batch_y in iterator.get_batches(train_set, shuffle=True):\n",
    "    loss, outputs = trainer.train_on_batch(batch_X, batch_y)\n",
    "    loss_list = np.append(loss_list, loss.detach().cpu().numpy())\n",
    "#     try:\n",
    "#         outputs_list\n",
    "#         outputs_list = np.dstack((outputs_list, outputs.detach().cpu().numpy()))\n",
    "#     except NameError:\n",
    "#         outputs_list = outputs.detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "path = '/home/tanja/DLVR/savedInfo/'\n",
    "data_list = np.sort(glob.glob(path + 'data_*'))\n",
    "weights_list = np.sort(glob.glob(path + 'weights_*'))\n",
    "grad_list = np.sort(glob.glob(path + 'grad_*'))\n",
    "optimizer_list = np.sort(glob.glob(path + 'optimizer_*'))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = braindecode.models.deep4.Deep4Net(in_chans=parameters['in_chans'],\n",
    "#                                           n_classes=parameters['n_classes'],\n",
    "#                                           input_time_length=parameters['input_time_length'], \n",
    "#                                           final_conv_length=1, \n",
    "#                                           batch_norm_alpha=0.1).create_network()\n",
    "# to_dense_prediction_model(model)\n",
    "# model.cuda()\n",
    "model = torch.load('model_19-06_11-16')\n",
    "\n",
    "layer_names = list(dict(model.named_children()).keys())\n",
    "\n",
    "optimizer = AdamW(model_cropp.parameters(), \n",
    "                  lr = parameters['lr'], weight_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_check = np.array([])\n",
    "for item in range(len(weights_list)):\n",
    "    \n",
    "    batch = torch.load(data_list[item])\n",
    "    inputs = Variable(FloatTensor(batch['inputs']))\n",
    "    targets = Variable(LongTensor(batch['targets']))\n",
    "    \n",
    "    model.train()\n",
    "    weight_step = torch.load(weights_list[item])\n",
    "    grad_step = torch.load(grad_list[item])    \n",
    "    optimizer_step = torch.load(optimizer_list[item])\n",
    "    \n",
    "    for l_nr, layer in enumerate(model):\n",
    "        if hasattr(layer, 'weight'):\n",
    "            layer.weight = weight_step[layer_names[l_nr] + '_weight']\n",
    "            layer.weight.grad = grad_step[layer_names[l_nr] + '_weight_grad']\n",
    "        if hasattr(layer, 'bias'):\n",
    "            layer.bias = weight_step[layer_names[l_nr] + '_bias']\n",
    "            if hasattr(layer.bias, 'grad'):\n",
    "                layer.bias.grad = grad_step[layer_names[l_nr] + '_bias_grad'] \n",
    "    \n",
    "    outputs = model(inputs)\n",
    "    outputs = torch.mean(outputs, dim=2, keepdim=False)\n",
    "    loss = criterion(outputs, targets)\n",
    "#     optimizer = AdamW(model.parameters(), \n",
    "#                   lr = parameters_online['lr'], weight_decay=0)\n",
    "#     optimizer.load_state_dict(optimizer_step)\n",
    "#     optimizer.step()\n",
    "    \n",
    "    loss_check = np.append(loss_check, loss.cpu().detach().numpy())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process = OnlineToOffline.OfflineProcess(model_cropp, data=data_list, weights=weights_list, gradients=grad_list, optimizer_state=optimizer_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autotraining\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "\n",
    "from braindecode.torch_ext.util import set_random_seeds\n",
    "from braindecode.models.util import to_dense_prediction_model\n",
    "\n",
    "cuda = True\n",
    "set_random_seeds(seed=20170629, cuda=cuda)\n",
    "\n",
    "input_time_length = train.shape[2]\n",
    "n_classes = 2\n",
    "in_chans = train.shape[1]\n",
    "model_cropp = braindecode.models.deep4.Deep4Net(in_chans=in_chans, n_classes=n_classes, \n",
    "                 input_time_length=input_time_length, final_conv_length=1, \n",
    "                       batch_norm_alpha=0.1)\n",
    "\n",
    "if cuda:\n",
    "    model_cropp.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model_cropp.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "model_cropp.compile(loss=F.nll_loss, optimizer=optimizer, iterator_seed=1, cropped=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_time_length = train.shape[2]\n",
    "model_cropp.fit(train_set.X, train_set.y, epochs=100, batch_size=60, scheduler='cosine', input_time_length=input_time_length, validation_data=(valid_set.X, valid_set.y),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cropp.epochs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cropp.evaluate(test_set.X, test_set.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(model_cropp.epochs_df['train_loss'], label='training loss')\n",
    "plt.plot(model_cropp.epochs_df['valid_loss'], label='validation loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(model_cropp.epochs_df['train_misclass'], label='training misclass')\n",
    "plt.plot(model_cropp.epochs_df['valid_misclass'], label='validation misclass')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
